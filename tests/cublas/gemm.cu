#include <cuda.h>
#include <cuda_fp16.h>

// generated by TVM
__global__ void tvm_gemm_kernel(const signed char *__restrict__ A,
                                const signed char *__restrict__ B,
                                int *__restrict__ compute) {
  int compute_local[8];
  __shared__ signed char A_shared[4352];
  __shared__ signed char B_shared[4096];
  signed char A_shared_local[16];
  signed char B_shared_local[128];
  for (int j_c_init = 0; j_c_init < 8; ++j_c_init) {
    compute_local[(j_c_init)] = 0;
  }
  for (int k_outer = 0; k_outer < 16; ++k_outer) {
    __syncthreads();
    for (int ax0_ax1_outer_fused_outer = 0; ax0_ax1_outer_fused_outer < 8;
         ++ax0_ax1_outer_fused_outer) {
      ((int4 *)(A_shared + (((((ax0_ax1_outer_fused_outer * 544) +
                               ((((int)threadIdx.y) >> 3) * 272)) +
                              ((((int)threadIdx.y) & 7) * 32)) +
                             (((int)threadIdx.x) * 16)))))[0] =
          ((int4 *)(A + (((((((((int)blockIdx.y) * 65536) +
                              (ax0_ax1_outer_fused_outer * 8192)) +
                             ((((int)threadIdx.y) >> 3) * 4096)) +
                            (k_outer * 256)) +
                           ((((int)threadIdx.y) & 7) * 32)) +
                          (((int)threadIdx.x) * 16)))))[0];
    }
    for (int ax0_ax1_outer_fused_outer1 = 0; ax0_ax1_outer_fused_outer1 < 8;
         ++ax0_ax1_outer_fused_outer1) {
      ((int4 *)(B_shared + ((((ax0_ax1_outer_fused_outer1 * 512) +
                              (((int)threadIdx.y) * 32)) +
                             (((int)threadIdx.x) * 16)))))[0] =
          ((int4 *)(B + (((((((((int)blockIdx.x) * 65536) +
                              (ax0_ax1_outer_fused_outer1 * 8192)) +
                             ((((int)threadIdx.y) >> 3) * 4096)) +
                            (k_outer * 256)) +
                           ((((int)threadIdx.y) & 7) * 32)) +
                          (((int)threadIdx.x) * 16)))))[0];
    }
    __syncthreads();
    for (int k_inner_outer = 0; k_inner_outer < 16; ++k_inner_outer) {
      for (int ax1 = 0; ax1 < 16; ++ax1) {
        A_shared_local[(ax1)] = A_shared[(
            (((((int)threadIdx.y) * 272) + (k_inner_outer * 16)) + ax1))];
      }
      for (int ax0 = 0; ax0 < 8; ++ax0) {
        for (int ax11 = 0; ax11 < 16; ++ax11) {
          B_shared_local[(((ax0 * 16) + ax11))] =
              B_shared[(((((((int)threadIdx.x) * 2048) + (ax0 * 256)) +
                          (k_inner_outer * 16)) +
                         ax11))];
        }
      }
      for (int k_inner_inner = 0; k_inner_inner < 16; ++k_inner_inner) {
        for (int j_c = 0; j_c < 8; ++j_c) {
          compute_local[(j_c)] =
              (compute_local[(j_c)] +
               (((int)A_shared_local[(k_inner_inner)]) *
                ((int)B_shared_local[(((j_c * 16) + k_inner_inner))])));
        }
      }
    }
  }
  for (int j_inner_inner_inner = 0; j_inner_inner_inner < 8;
       ++j_inner_inner_inner) {
    compute[((((((((int)blockIdx.y) * 16384) + (((int)threadIdx.y) * 1024)) +
                (((int)blockIdx.x) * 16)) +
               (((int)threadIdx.x) * 8)) +
              j_inner_inner_inner))] = compute_local[(j_inner_inner_inner)];
  }
}

void launch_tvm_gemm(const int8_t *A, const int8_t *B, int32_t *C,
                     cudaStream_t &stream) {
  int grid_dim = 64;
  dim3 block_dim(2, 16, 1);
  tvm_gemm_kernel<<<grid_dim, block_dim, 0, stream>>>(A, B, C);
}
