syntax = "proto3";
option optimize_for = LITE_RUNTIME;
// all the matrix are stored in row-major order,
// plz see https://en.wikipedia.org/wiki/Row-_and_column-major_order for details

// the definition of "Multi-Head Attention", "Scaled Dot-Product Attention" and
// "Feed-Forward Networks"
// plz see https://arxiv.org/abs/1706.03762 for details

message QuantEncoderLayer {
  // encoder-self-attention
  repeated float multihead_norm_scale = 1;  // [hidden_size]
  repeated float multihead_norm_bias = 2;   // [hidden_size]
  // "Multi-Head Attention" linearly project weights kernel for query, key,
  // value,
  // before "Scaled Dot-Product Attention, with shape (hidden_size,
  // hidden_size*3)
  // is built by numpy.concatenate((query_kernel, key_kernel, value_kernel),
  // axis=1)
  // perform numpy.dot(input, multihead_project_kernel_qkv) will get the [query,
  // key, value] of
  // "Scaled Dot-Product Attention"
  bytes multihead_project_kernel_qkv =
      3;  // [hidden_size, 3, hidden_size]
  repeated float multihead_project_bias_qkv = 4;  // [3, hidden_size]
  // "Multi-Head Attention" linearly project weights kernel for output
  // after "Scaled Dot-Product Attention", with shape (hidden_size, hidden_size)
  bytes multihead_project_kernel_output =
      5;  // [hidden_size, hidden_size]
  repeated float multihead_project_bias_output = 6;  // [hidden_size]

  // "Feed-Forward Networks"
  repeated float ffn_norm_scale = 7;      // [hidden_size]
  repeated float ffn_norm_bias = 8;       // [hidden_size]
  bytes ffn_first_kernel = 9;    // [hidden_size, inner_size]
  repeated float ffn_first_bias = 10;     // [inner_size]
  bytes ffn_second_kernel = 11;  // [inner_size, hidden_size]
  repeated float ffn_second_bias = 12;    // [hidden_size]

  // clip max
  float multihead_project_kernel_qkv_clip_max = 13;
  float multihead_project_kernel_output_clip_max = 14;
  float ffn_first_kernel_clip_max = 15;
  float ffn_second_kernel_clip_max = 16;
  float multihead_ln_clip_max = 17;
  float multihead_project_output_clip_max = 18;
  float ffn_ln_clip_max = 19;
  float ffn_first_act_clip_max = 20;
  float multihead_qkv_dense_clip_max = 21;
  float multihead_output_dense_clip_max = 22;
  float ffn_first_output_clip_max = 23;
  float ffn_second_output_clip_max = 24;
}

message QuantDecoderLayer {
  // decoder-self-attention
  repeated float self_norm_scale = 1;          // [hidden_size]
  repeated float self_norm_bias = 2;           // [hidden_size]
  bytes self_project_kernel_qkv = 3;  // [hidden_size, 3, hidden_size]
  repeated float self_project_bias_qkv = 4;    // [3, hidden_size]
  bytes self_project_kernel_output = 5;  // [hidden_size, hidden_size]
  repeated float self_project_bias_output = 6;    // [hidden_size]

  // decoder-encode-attention
  repeated float encdec_norm_scale = 7;        // [hidden_size]
  repeated float encdec_norm_bias = 8;         // [hidden_size]
  bytes encdec_project_kernel_q = 9;  // [hidden_size, hidden_size]
  repeated float encdec_project_bias_q = 10;   // [hidden_size]
  bytes encdec_project_kernel_output =
      11;                                          // [hidden_size, hidden_size]
  repeated float encdec_project_bias_output = 12;  // [hidden_size]

  // "Feed-Forward Networks"
  repeated float ffn_norm_scale = 13;     // [hidden_size]
  repeated float ffn_norm_bias = 14;      // [hidden_size]
  bytes ffn_first_kernel = 15;   // [hidden_size, inner_size]
  repeated float ffn_first_bias = 16;     // [inner_size]
  bytes ffn_second_kernel = 17;  // [inner_size, hidden_size]
  repeated float ffn_second_bias = 18;    // [hidden_size]

  // clip max
  float self_project_kernel_qkv_clip_max = 19;
  float self_project_kernel_output_clip_max = 20;
  float encdec_project_kernel_q_clip_max = 21;
  float encdec_project_kernel_output_clip_max = 22;
  float ffn_first_kernel_clip_max = 23;
  float ffn_second_kernel_clip_max = 24;
  float self_ln_clip_max = 25;
  float self_project_output_clip_max = 26;
  float encdec_ln_clip_max = 27;
  float encdec_project_output_clip_max = 28;
  float ffn_ln_clip_max = 29;
  float ffn_first_act_clip_max = 30;
  float self_qkv_dense_clip_max = 31;
  float self_output_dense_clip_max = 32;
  float encdec_q_dense_clip_max = 33;
  float encdec_output_dense_clip_max = 34;
  float ffn_first_output_clip_max = 35;
  float ffn_second_output_clip_max = 36;
  float self_qkv_bias_out_clip_max = 37;
}

message QuantEmbeddingLayer {
  // token embedding table
  // for encoder, it is in [src_vocab_size, hidden_size]
  // for decoder, it is in [hidden_size, trg_vocab_size]
  // notice, it shoule have been multiply by sqrt(hidden_size)
  // so, look it up directly will get the input token embedding, there is no
  // need
  // to multiply by sqrt(hidden_size) during inference.
  bytes token_embedding = 1;
  repeated float position_embedding = 2;  // [max_step, hidden_size]
  // the last layer_norm of encoder or decoder
  repeated float norm_scale = 3;  // [hidden_size]
  repeated float norm_bias = 4;   // [hidden_size]

  // only for trg, not in src
  // [dec_layer_num, hidden_size, 2, hidden_size]
  bytes encode_output_project_kernel_kv = 5;
  // only for trg, not in src
  // [dec_layer_num, 2, hidden_size]
  repeated float encode_output_project_bias_kv = 6;
  // only for trg, not in src
  // decoder vocab logit bias
  repeated float shared_bias = 7;  // [target_vocab_size]

  // For multi lingual model, [num_lang, hidden_size]
  repeated float lang_emb = 8;

  // clip max
  float emb_clip_max = 9;
  repeated float encode_output_project_kernel_kv_clip_max = 10;
  float output_ln_clip_max = 11;
  float logits_clip_max = 12;
}

message QuantModelConf {
  int32 head_num = 1;   // head number for multi-head attention
  int32 beam_size = 2;  // beam size of beam search
  int32 extra_decode_length =
      3;                     // extra decode length compared with source length
  float length_penalty = 4;  // length penalty of beam search
  int32 src_padding_id = 5;  // source padding id
  int32 trg_start_id = 6;    // target start id
  float diverse_lambda = 7; // diverse beam search lambda
  string sampling_method = 8; // choice of beam_search, topk, topp, topk_greedy
  float topp = 9; // parameter for topp sampling
  int32 topk = 10; // parameter for topk sampling
  int32 trg_end_id = 11; // eos of target embedding
  bool is_post_ln = 12; // Pre-LN or Post-LN
  bool no_scale_embedding = 13; // whether to scale embedding by sqrt(emb_dim)
  bool use_gelu = 14; // use gelu for activation otherwise relu
  // Multilingual model type, 0 for bilingual
  // 1 for token level multilingual,
  // 2 for sentence level multilingual
  int32 multilg_type = 15;
}

message QuantTransformer {
  QuantEmbeddingLayer src_embedding = 1;         // source embedding
  repeated QuantEncoderLayer encoder_stack = 2;  // encoder weights
  QuantEmbeddingLayer trg_embedding = 3;         // target embedding
  repeated QuantDecoderLayer decoder_stack = 4;  // decoder weighs
  QuantModelConf model_conf = 5;                 // model_config
}
