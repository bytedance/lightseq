syntax = "proto3";
option optimize_for = LITE_RUNTIME;
// all the matrix are stored in row-major order,
// plz see https://en.wikipedia.org/wiki/Row-_and_column-major_order for details

// the definition of "Multi-Head Attention", "Scaled Dot-Product Attention" and
// "Feed-Forward Networks"
// plz see https://arxiv.org/abs/1706.03762 for details

message GptEncoderLayer {
  // layer norm before "Multi-Head Attention"
  repeated float multihead_norm_scale = 1;
  repeated float multihead_norm_bias = 2;

  // "Multi-Head Attention" linearly project weights kernel for query, key,
  // value,
  // before "Scaled Dot-Product Attention, with shape (hidden_size,
  // hidden_size*3)
  // is built by numpy.concatenate((query_kernel, key_kernel, value_kernel),
  // axis=1)
  // perform numpy.dot(input, multihead_project_kernel_qkv) will get the [query,
  // key, value] of
  // "Scaled Dot-Product Attention"
  repeated float multihead_project_kernel_qkv = 3;
  repeated float multihead_project_bias_qkv = 4;
  // "Multi-Head Attention" linearly project weights kernel for output
  // after "Scaled Dot-Product Attention", with shape (hidden_size, hidden_size)
  repeated float multihead_project_kernel_output = 5;
  repeated float multihead_project_bias_output = 6;

  // layer norm before "Feed-Forward Networks"
  repeated float ffn_norm_scale = 7;
  repeated float ffn_norm_bias = 8;

  // "Feed-Forward Networks"
  repeated float ffn_first_kernel = 9;
  repeated float ffn_first_bias = 10;
  repeated float ffn_second_kernel = 11;
  repeated float ffn_second_bias = 12;
}

message GptEmbeddingLayer {
  // token embedding table
  // for encoder, it is in [src_vocab_size, hidden_size]
  // so, look it up directly will get the input token embedding
  repeated float token_embedding = 1;
  repeated float position_embedding = 2;
  // the last layer_norm of encoder
  repeated float norm_scale = 3;
  repeated float norm_bias = 4;
}

message GptModelConf {
  int32 head_num = 1;
  int32 src_padding_id = 2;
  string sampling_method = 3;
  float topp = 4;
  int32 topk = 5;
  int32 eos_id = 6;
  int32 extra_decode_length = 7;
}

message Gpt {
  GptEmbeddingLayer src_embedding = 1;
  repeated GptEncoderLayer encoder_stack = 2;
  GptModelConf model_conf = 3;
}
