syntax = "proto3";
option optimize_for = LITE_RUNTIME;
// all the matrix are stored in row-major order, 
// plz see https://en.wikipedia.org/wiki/Row-_and_column-major_order for details

// the definition of "Multi-Head Attention", "Scaled Dot-Product Attention" and "Feed-Forward Networks"
// plz see https://arxiv.org/abs/1706.03762 for details

message EncoderLayer {
    // layer norm before "Multi-Head Attention"
    repeated float multihead_norm_scale = 1;
    repeated float multihead_norm_bias = 2;
    
    // "Multi-Head Attention" linearly project weights kernel for query, key, value, 
    // before "Scaled Dot-Product Attention, with shape (hidden_size, hidden_size*3)
    // is built by numpy.concatenate((query_kernel, key_kernel, value_kernel), axis=1)
    // perform numpy.dot(input, multihead_project_kernel_qkv) will get the [query, key, value] of
    // "Scaled Dot-Product Attention"
    repeated float multihead_project_kernel_qkv = 3;
    repeated float multihead_project_bias_qkv = 4;
    // "Multi-Head Attention" linearly project weights kernel for output
    // after "Scaled Dot-Product Attention", with shape (hidden_size, hidden_size)
    repeated float multihead_project_kernel_output = 5;
    repeated float multihead_project_bias_output = 6;

    
    // layer norm before "Feed-Forward Networks"
    repeated float ffn_norm_scale = 7;
    repeated float ffn_norm_bias = 8;
    
    // "Feed-Forward Networks"
    repeated float ffn_first_kernel = 9;
    repeated float ffn_first_bias = 10;
    repeated float ffn_second_kernel = 11;
    repeated float ffn_second_bias = 12;
}

message EmbeddingLayer {
    // token embedding table
    // for encoder, it is in [src_vocab_size, hidden_size]
    // so, look it up directly will get the input token embedding
    repeated float token_embedding = 1;
    repeated float position_embedding = 2;
    // the last layer_norm of encoder
    repeated float norm_scale = 3; 
    repeated float norm_bias = 4; 
}

message ModelConf {
    int32 head_num = 1;
    int32 src_padding_id = 2;
}

message Gpt {
    EmbeddingLayer src_embedding = 1;
    repeated EncoderLayer encoder_stack = 2;
    ModelConf model_conf = 3;
}
